<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description"
    content="This is a project page for Multimodal Cross-Domain Few-Shot Learning for Egocentric Action Recognition">
  <meta property="og:title" content="Multimodal Cross-Domain Few-Shot Learning for Egocentric Action Recognition">
  <meta property="og:description"
    content="This is a project page for Multimodal Cross-Domain Few-Shot Learning for Egocentric Action Recognition">
  <meta property="og:url" content="https://masashi-hatano.github.io/MM-CDFSL/">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <!-- <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/> -->


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords"
    content="masashi, hatano, keio, egocentric, mm-cdfsl, cross-domain, few-shot, action recognition">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Multimodal Cross-Domain Few-Shot Learning for Egocentric Action Recognition</title>
  <!-- <link rel="icon" type="image/x-icon" href="static/images/favicon.ico"> -->
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>

<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Multimodal Cross-Domain Few-Shot Learning for Egocentric Action
              Recognition</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://masashi-hatano.github.io/" target="_blank">Masashi Hatano</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://ryohachiuma.github.io/" target="_blank">Ryo Hachiuma</a><sup>2</sup>,</span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=ocx_zMcAAAAJ&hl=en" target="_blank">Ryo
                  Fuji</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://scholar.google.co.jp/citations?user=JU9x-bcAAAAJ&hl=en&oi=ao" target="_blank">Hideo
                  Saito</a><sup>1</sup>
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block">Keio University<sup>1</sup>, NVIDIA<sup>2</sup><br><em>Under Review</em></span>
              <!-- <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span> -->
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- Arxiv PDF link -->
                <span class="link-block">
                  <a href="https://masashi-hatano.github.io/assets/pdf/mm-cdfsl.pdf" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>

                <!-- Supplementary PDF link -->
                <span class="link-block">
                  <a href="https://masashi-hatano.github.io/assets/pdf/mm-cdfsl_supp.pdf" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Supplementary</span>
                  </a>
                </span>

                <!-- Github link -->
                <span class="link-block">
                  <a href="https://github.com/masashi-hatano/MM-CDFSL" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- Teaser video-->
  <!-- <section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted loop height="100%"> -->
  <!-- Your video here -->
  <!-- <source src="static/videos/supplementary.mp4"
        type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
      </h2>
    </div>
  </div>
</section> -->
  <!-- End teaser video -->
  <!-- Image Approach -->
  <section class="section hero is-small">
    <div class="container is-max-desktop">
      <img src="static/images/dataset.jpg" alt="MY ALT TEXT" />
    </div>
  </section>
  <!-- End image Approach -->

  <!-- Paper abstract -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              We address a novel cross-domain few-shot learning task (CD-FSL) with multimodal input and unlabeled target
              data for egocentric action recognition.
              This paper simultaneously tackles two critical chal-lenges associated with egocentric action recognition
              in CD-FSL settings:(1) the extreme domain gap in egocentric videos (e.g., daily life vs. indus-trial
              domain) and (2) the computational cost for real-world applications.
              We propose MM-CDFSL, a domain-adaptive and computationally effi-cient approach designed to enhance
              adaptability to the target domain and improve inference speed.
              To address the first challenge, we propose the incorporation of multimodal distillation into the student
              RGB model using teacher models.
              Each teacher model is trained independently on source and target data for its respective modality.
              Leveraging only un-labeled target data during multimodal distillation enhances the student model's
              adaptability to the target domain.
              We further introduce ensem-ble masked inference, a technique that reduces the number of input to-kens
              through masking.
              In this approach, ensemble prediction mitigates the performance degradation caused by masking, effectively
              address-ing the second issue.
              Our approach outperformed the state-of-the-art CD-FSL approaches with a substantial margin on multiple
              egocentric datasets, improving by an average of 6.12/6.10 points for 1-shot/5-shot settings while
              achieving 2.2 times faster inference speed.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End paper abstract -->


  <!-- Image Approach -->
  <section class="section hero is-small">
    <div class="container is-max-desktop">
      <h2 class="title is-3">Approach</h2>
      <img src="static/images/mm-cdfsl.jpg" alt="MY ALT TEXT" />
    </div>
  </section>
  <!-- End image Approach -->

  <!-- Youtube video -->
  <!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container"> -->
  <!-- Paper video. -->
  <!-- <h2 class="title is-3">Video Presentation</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          
          <div class="publication-video"> -->
  <!-- Youtube embed code here -->
  <!-- <iframe src="https://www.youtube.com/embed/JkaxUblCGz0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
        </div>
      </div>
    </div>
  </div>
</section> -->
  <!-- End youtube video -->


  <!-- Video carousel -->
  <!-- <section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Another Carousel</h2>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-video1">
          <video poster="" id="video1" autoplay controls muted loop height="100%"> -->
  <!-- Your video file here -->
  <!-- <source src="static/videos/carousel1.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video2">
          <video poster="" id="video2" autoplay controls muted loop height="100%"> -->
  <!-- Your video file here -->
  <!-- <source src="static/videos/carousel2.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video3">
          <video poster="" id="video3" autoplay controls muted loop height="100%">\ -->
  <!-- Your video file here -->
  <!-- <source src="static/videos/carousel3.mp4"
            type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section> -->
  <!-- End video carousel -->






  <!-- Paper poster -->
  <section class="hero is-small is-light">
    <div class="hero-body">
      <div class="container">
        <h2 class="title">Poster (coming soon)</h2>

        <!-- <iframe  src="static/pdfs/sample.pdf" width="100%" height="550">
          </iframe> -->

      </div>
    </div>
  </section>
  <!--End paper poster -->


  <!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>
        @article{Hatano2024MM-CDFSL,
          author = {Masashi Hatano and
                    Ryo Hachiuma and
                    Ryo Fuji and
                    Hideo Saito},
          title = {Multimodal Cross-Domain Few-Shot Learning for Egocentric Action Recognition},
          journal = {Under Review},
          year = {2024},
        }
      </code></pre>
    </div>
  </section>
  <!--End BibTex citation -->


  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">

            <p>
              This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template"
                target="_blank">Academic Project Page Template</a> which was adopted from the <a
                href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
              You are free to borrow the of this website, we just ask that you link back to this page in the footer.
              <br> This website is licensed under a <a rel="license"
                href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
                Commons Attribution-ShareAlike 4.0 International License</a>.
            </p>

          </div>
        </div>
      </div>
    </div>
  </footer>

  <!-- Statcounter tracking code -->

  <!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

  <!-- End of Statcounter Code -->

</body>

</html>